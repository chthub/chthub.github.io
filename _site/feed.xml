<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Everyday Paper</title>
    <description>every day paper</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sat, 24 Sep 2022 10:29:27 -0400</pubDate>
    <lastBuildDate>Sat, 24 Sep 2022 10:29:27 -0400</lastBuildDate>
    <generator>Jekyll v4.2.2</generator>
    
      <item>
        <title>GNNExplainer: Generating Explanations for Graph Neural Networks</title>
        <description>&lt;p&gt;Published on NeurIPS 2019&lt;/p&gt;

&lt;p&gt;本文想要解释GNN的预测结果。如下图所示，对于节点 $v$， 解释$v$的预测结果分成了两部分，一个是解释$v$周围的子图，哪些节点和边对预测结果重要，哪些不重要。一部分是解释节点的特征，哪些维度的特征发挥了作用，哪些没有。&lt;/p&gt;

&lt;p&gt;本文基于互信息最大化的框架，学习邻接矩阵和节点特征的mask，实现对预测结果的可解释性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/chthub.github.io/main/imgs/image-20220924101937523.png&quot; alt=&quot;image-20220924101937523&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;formulation&quot;&gt;Formulation&lt;/h2&gt;

&lt;p&gt;首先是一个关键的insight，当预测节点的标签时，GNN基于的是节点周围的邻居信息（文章中称之为computation graph）。即节点$v$的计算图$G_c(v)$告诉了GNN如何生成其embedding $\mathbf{z}$。计算图$G_c(v)$的邻接矩阵是$A_c(v) \in\lbrace 0,1\rbrace^{n \times n}$，节点的特征集合是$X_c(v)=\left\lbrace x_j \mid v_j \in G_c(v)\right\rbrace$。然后可以将GNN $\Phi$基于计算图的计算过程表示为：&lt;/p&gt;

\[P_{\Phi}\left(Y \mid G_c, X_c\right)\]

&lt;p&gt;$Y\in \lbrace 1, \ldots, C\rbrace$表示节点的标签。&lt;/p&gt;

&lt;p&gt;本文提出的GNNExplainer会对预测$\hat{y}$生成解释$(G_S,X_S^F)$。$G_S$是计算图的子图，$X_S$是对应的节点特征集合，$X_S^F$是节点的特征子集，由$F$进行mask。GNNExplainer可以用于单实例解释和多实例解释的情况。&lt;/p&gt;

&lt;h3 id=&quot;单实例解释&quot;&gt;单实例解释&lt;/h3&gt;

&lt;p&gt;给定节点 $v$, 我们的目标是识别对预测结果 $\hat{y}$ 重要的子图 $G_S \subseteq G_c$ 和关联的特征 $X_S=$ $\left\lbrace x_j \mid v_j \in G_S\right\rbrace$。我们使用互信息来建模这种重要性：&lt;/p&gt;

\[\max _{G_S} M I\left(Y,\left(G_S, X_S\right)\right)=H(Y)-H\left(Y \mid G=G_S, X=X_S\right)\]

&lt;p&gt;当GNN训练好之后$\Phi$是固定的，所以第一项为常数。所以最大化互信息等价于最小化条件熵：&lt;/p&gt;

\[H\left(Y \mid G=G_S, X=X_S\right)=-\mathbb{E}_{Y \mid G_S, X_S}\left[\log P_{\Phi}\left(Y \mid G=G_S, X=X_S\right)\right]\]

&lt;p&gt;如果 $G_S \sim \mathcal{G}$ 是随机的图变量，此时：&lt;/p&gt;

\[\min _{\mathcal{G}} \mathbb{E}_{G_S \sim \mathcal{G}} H\left(Y \mid G=G_S, X=X_S\right)\]

&lt;p&gt;在凸假设下，基于Jensen不等式有如下的上界：&lt;/p&gt;

\[\min _{\mathcal{G}} H\left(Y \mid G=\mathbb{E}_{\mathcal{G}}\left[G_S\right], X=X_S\right)\]

&lt;p&gt;然后这里有一些基于平均场变分近似的讨论，没看懂。&lt;/p&gt;

&lt;p&gt;如果要回答诸如：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;why does the trained model predict a certain class label&lt;/p&gt;

  &lt;p&gt;or&lt;/p&gt;

  &lt;p&gt;how to make the trained model predict a desired class label&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;可以利用交叉熵修改损失函数如下：&lt;/p&gt;

\[\min _M-\sum_{c=1}^C \mathbb{1}[y=c] \log P_{\Phi}\left(Y=y \mid G=A_c \odot \sigma(M), X=X_c\right)\]

&lt;p&gt;这里的$M$就是mask 矩阵，$\odot$表示逐元素的乘积。&lt;/p&gt;

&lt;p&gt;为了学习哪些节点特征对预测 $\hat{y}$ 是重要的，GNNEXPLAINER学习一个特征选择器$F$。这里的$F$是一个mask向量。所以最终的目标函数如下：&lt;/p&gt;

\[\max _{G_S, F} M I\left(Y,\left(G_S, F\right)\right)=H(Y)-H\left(Y \mid G=G_S, X=X_S^F\right)\]

</description>
        <pubDate>Sat, 24 Sep 2022 08:00:00 -0400</pubDate>
        <link>http://localhost:4000/2022/09/24/GNNExplainer/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/09/24/GNNExplainer/</guid>
        
        <category>GNN</category>
        
        <category>可解释性</category>
        
        <category>NeurIPS</category>
        
        
      </item>
    
      <item>
        <title>Multi-level Distance Regularization for Deep Metric Learning</title>
        <description>&lt;p&gt;Published on AAAI 2021&lt;/p&gt;

&lt;p&gt;本文提出了一种用于深度度量学习的新的基于距离的正则化方法，称为Multi-level Distance Regularization (MDR)。它能够让表征空间中向量之间的距离归属于不同的level，以表示样本对之间不同程度的相似性。在训练过程中，这种多层级的设计能够阻止样本被忽视（因为样本太简单了）或者被过度注意（因为样本太难了），这使得参数的优化过程更加平稳，同时提升了模型的泛化能力。&lt;/p&gt;

&lt;p&gt;下图是本文的损失和Triplet loss的对比：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/chthub.github.io/main/imgs/image-20220919100848434.png&quot; alt=&quot;image-20220919100848434&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;formulation&quot;&gt;Formulation&lt;/h2&gt;

&lt;p&gt;本文提出的Multi-level Distance Regularization（MDR）可以分成3步。&lt;/p&gt;

&lt;h3 id=&quot;1-distance-normalization&quot;&gt;1. Distance Normalization&lt;/h3&gt;

&lt;p&gt;对于输入图片$x$，编码网络 $f$ 将其映射为表征向量 $e$：&lt;/p&gt;

\[e=f(x)\]

&lt;p&gt;可以基于欧式距离定义两个表征向量之间的距离：&lt;/p&gt;

\[d\left(e_i, e_j\right)=\left\|e_i-e_j\right\|_2\]

&lt;p&gt;然后对距离进行标准化：&lt;/p&gt;

\[\bar{d}\left(e_i, e_j\right)=\frac{d\left(e_i, e_j\right)-\mu}{\sigma}\]

&lt;p&gt;基于样本对的集合$\mathcal{P}=\left\lbrace\left(e_i, e_j\right) \mid i \neq j\right\rbrace$ ，$\mu$是距离的均值，$\sigma$是距离的标准差。为了考虑到整个数据集，均值和标准差的更新可以采用动量更新的方式：&lt;/p&gt;

\[\begin{aligned}
\mu_t^* &amp;amp;=\gamma \mu_{t-1}^*+(1-\gamma) \mu \\
\sigma_t^* &amp;amp;=\gamma \sigma_{t-1}^*+(1-\gamma) \sigma
\end{aligned}\]

&lt;p&gt;此时标准化之后的距离可以写成：&lt;/p&gt;

\[\bar{d}\left(e_i, e_j\right)=\frac{d\left(e_i, e_j\right)-\mu^*}{\sigma^*} .\]

&lt;h3 id=&quot;2-level-assignment&quot;&gt;2. Level Assignment&lt;/h3&gt;

&lt;p&gt;MDR将不同的level作为标准化之后的距离的正则化目标。预先定义一个level的集合$s\in\mathcal{S}$，集合里面的level值初始化为预先定义的数值，比如$\mathcal{S}=\lbrace-3,0,3\rbrace$。每个level $s$可以理解成对距离标准差的乘数。$g(d;s)$是一个指示函数，用来判断是否某个距离 $d$ 与给定的level $s$ 是最近的：&lt;/p&gt;

\[g(d, s)= \begin{cases}1, &amp;amp; \text { if } \arg \min _{s_i \in \mathcal{S}}\left|d-s_i\right| \text { is } s \\ 0, &amp;amp; \text { otherwise }\end{cases}\]

&lt;h3 id=&quot;3-regularization&quot;&gt;3. Regularization&lt;/h3&gt;

&lt;p&gt;最终的损失函数定义如下：&lt;/p&gt;

\[\mathcal{L}_{\mathrm{MDR}}=\frac{1}{\mathcal{P}} \sum_{\left(e_i, e_j\right) \in \mathcal{P}} \sum_{s \in \mathcal{S}} g\left(\bar{d}\left(e_i, e_j\right), s\right) \cdot\left|\bar{d}\left(e_i, e_j\right)-s\right| .\]

&lt;p&gt;这里的level $s$ 是可学习的参数，作者在实验中尝试了对level set不同的初始化，发现$\lbrace-3,0,3\rbrace$是一个比较好的初始化。可以看到，这个在计算损失的时候，如果$e_i,e_j$之间的距离不属于某个特定的level，则不计算之间的距离和level之间的损失，计算的是距离最近的level之间的损失。&lt;/p&gt;

&lt;p&gt;这个损失函数有两个作用：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;与之间对两个样本之间距离进行优化的方法相比，这种对距离的多层次分配使得优化变得更加复杂，这就防止模型在训练集上产生过拟合。&lt;/li&gt;
  &lt;li&gt;最外边的level阻止了和正样本对之间变得更近，和负样本对之间变得更远。这提升了学习难度，使得训练过程更加平稳，对简单的样本loss下降不会太快，也不会太偏向于困难的样本。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在最终学习时的损失函数可以用任意的深度度量学习函数和本文提出的MDR组合：&lt;/p&gt;

\[\mathcal{L}=\mathcal{L}_{\mathrm{DML}}+\lambda \mathcal{L}_{\mathrm{MDR}} .\]

&lt;p&gt;下图是模型的框架图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/chthub.github.io/main/imgs/image-20220919101004894.png&quot; alt=&quot;image-20220919101004894&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;embedding-normalization-trick-for-mdr&quot;&gt;Embedding Normalization Trick for MDR&lt;/h3&gt;

&lt;p&gt;作者提到，$L_2$ norm会扰乱MDR的正则化效果，但是为了对参数进行约束，文章中对$\mathcal{L}_{DML}$进行标准化，即对表征向量除以$\mu$，使得：&lt;/p&gt;

\[\mathbb{E}\left[d\left(\frac{e_i}{\mu}, \frac{e_j}{\mu}\right)\right]=1\]

&lt;h2 id=&quot;实验&quot;&gt;实验&lt;/h2&gt;

&lt;p&gt;模型在训练和测试集上的学习曲线如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/chthub.github.io/main/imgs/image-20220919095812735.png&quot; alt=&quot;image-20220919095812735&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到除了MDR之外的其他两种方法都在训练集上过拟合了，使得其在测试集上的效果下降。&lt;/p&gt;

&lt;p&gt;下面这张图可视化了类中心，可以看到MDR方法学到的类中心间距更均匀，距离更大。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/chthub.github.io/main/imgs/image-20220919095952090.png&quot; alt=&quot;image-20220919095952090&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下图是不同level的图片展示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/chthub.github.io/main/imgs/image-20220919100301273.png&quot; alt=&quot;image-20220919100301273&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到正样本对主要分布在level 1和level 2，负样本对主要分布在level 2和level 3。但是也有hard-positive pairs属于level 3，hard-negative pairs属于 level 1。可以看到，在这里的训练过程中，不在是那种binary supervision主导训练，而是level占主导。这就提升了模型的泛化能力。&lt;/p&gt;
</description>
        <pubDate>Mon, 19 Sep 2022 08:00:00 -0400</pubDate>
        <link>http://localhost:4000/2022/09/19/multilevel-distance/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/09/19/multilevel-distance/</guid>
        
        <category>度量学习</category>
        
        <category>AAAI</category>
        
        
      </item>
    
      <item>
        <title>Hypergraph-Induced Semantic Tuplet Loss for Deep Metric Learning</title>
        <description>&lt;p&gt;Published on CVPR 2022&lt;/p&gt;

&lt;p&gt;作者在这篇文章中主打的一个点是：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;relations among samples from the same class and similar-looking samples from different classes, must be helpful for understanding class-discriminative visual semantics, leading to improved feature learning&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;所以要考虑到一个样本同时和不同的类别有联系。比如对于鸟类识别任务来说，这种任务是比较精细的任务，粗略来说都是识别鸟，每个类别之间是有联系的，类别之间可能会有相似的样本。这是一种多边关系（multilateral relations），而超图正好可以建模这种任务。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/chthub.github.io/main/imgs/image-20220917154436171.png&quot; alt=&quot;image-20220917154436171&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;相关工作&quot;&gt;相关工作&lt;/h2&gt;

&lt;p&gt;目前已有的度量学习损失函数一种有4类：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Pair-based losses&lt;/li&gt;
  &lt;li&gt;Proxy-based losses&lt;/li&gt;
  &lt;li&gt;Classification-based losses&lt;/li&gt;
  &lt;li&gt;Graph-based losses&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这里每一类的相关工作还需要进一步看论文，先在这挖个坑。&lt;/p&gt;

&lt;h2 id=&quot;formulation&quot;&gt;Formulation&lt;/h2&gt;

&lt;p&gt;假设现在有$N$张图片$\mathcal{X}=\left\lbrace \mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_N\right\rbrace $ ，和对应的标签 $\mathcal{Y}=\left\lbrace y_1, y_2, \cdots, y_N\right\rbrace $，共有$C$类，$y_i \in\lbrace 1,2, \cdots, C\rbrace $。对于输入图片$\mathbf{x}_i$，有编码器$E$将其映射为$\mathbf{z}_i \in \mathbb{R}^D$：&lt;/p&gt;

\[\mathbf{z}_i=E\left(\mathbf{x}_i ; \boldsymbol{\Theta}\right)\]

&lt;p&gt;本文的目标是训练$E$使其产生有判别力的embedding。在接下来的算法设计里面，我们考虑的是一个由$N_b$个样本和标签组成的mini-batch $\mathcal{B}=\left\lbrace \left(\mathbf{x}_i, y_i\right)\right\rbrace _{i=1}^{N_b}$， $\mathcal{C} \subset\lbrace 1,2, \cdots, C\rbrace $ 表示 mini-batch $\mathcal{B}$ 的标签集合。&lt;/p&gt;

&lt;p&gt;然后我们定义原型分布（prototypical distributions），$\mathbb{D}=\left\lbrace \mathcal{D}_1, \mathcal{D}_2, \cdots, \mathcal{D}_C\right\rbrace $表示一组可学习的分布，共有$C$个，我们希望它能建模真实的特征分布。每一类的分布$\mathcal{D}_c$有两个参数，分别是均值 $\boldsymbol{\mu}_c \in \mathbb{R}^D$ 和协方差 $\mathbf{Q}_c \in \mathbb{R}^{D \times D}$, 分别表示类簇中心和类内的variations。为了计算上的简化，这里的$\mathbf{Q}_c$是对角矩阵，对角元是$\mathbf{q}_c\in \mathbb{R}^D$， $\mathbf{Q}_c=\operatorname{diag}\left(\mathbf{q}_c\right)$. 因此 $\mathbb{D}$ 中的所有参数可以表示为 $\boldsymbol{\Phi}=\left\lbrace \left(\boldsymbol{\mu}_c, \mathbf{q}_c\right)\right\rbrace _{c=1}^C$。&lt;/p&gt;

&lt;p&gt;然后可以定义分布损失$\mathcal{L}_D$。首先利用平方Mahalanobis距离计算样本$\mathbf{z}_i$和分布$\mathcal{D}_c$之间的距离：&lt;/p&gt;

\[d_m^2\left(\mathbf{z}_i, \mathcal{D}_c\right)=\left(\mathbf{z}_i-\boldsymbol{\mu}_c\right)^{\top} \mathbf{Q}_c^{-1}\left(\mathbf{z}_i-\boldsymbol{\mu}_c\right)\]

&lt;p&gt;基于此，可以定义$\mathbf{z}_ i$在其正确的原型分布$\mathcal{D}_+$上的概率：&lt;/p&gt;

\[P_i=\frac{\exp \left(-\tau d_m^2\left(\mathbf{z}_i, \mathcal{D}_{+}\right)\right)}{\sum_{\mathcal{D}_c \in \mathbb{D}} \exp \left(-\tau d_m^2\left(\mathbf{z}_i, \mathcal{D}_c\right)\right)}\]

&lt;p&gt;所以最终的损失可以写成所有样本的负对数似然的形式：&lt;/p&gt;

\[\mathcal{L}_D=\frac{1}{N_b} \sum_{i=1}^{N_b}-\log P_i\]

&lt;p&gt;我觉得这里有一个独特的地方是原型分布的均值$\boldsymbol{\mu}_c$是直接学出来的，而不是通过求平均值得到的。&lt;/p&gt;

&lt;p&gt;随后对于mini-batch $\mathcal{B}$中的每一类$c \in \mathcal{C}$构建一个语义元组（semantic tuplet）$\mathcal{S}(c)$ ，总共有$|\mathcal{C}|$个，基于原型分布$\mathbb{D}$, 语义元组可以表示为矩阵的形式 $\mathbf{S} \in[0,1]^{N_b \times|\mathcal{C}|}$：&lt;/p&gt;

\[\mathbf{S}_{i j}=\left\lbrace \begin{array}{cc}
1 &amp;amp; \text { if } y_i=\mathcal{C}_j \\
e^{-\alpha d_m^2\left(\mathbf{z}_i, \mathcal{D}_{\mathcal{C}_j}\right)} &amp;amp; \text { otherwise }
\end{array}\right.\]

&lt;p&gt;可以看到，当$\boldsymbol{S}_{ij}$不为1时，这可以看作是负样本的惩罚力度，越大说明就要去增加对这个样本的优化力度。此时从超图的角度来说，矩阵$\boldsymbol{S} _{ij}$可以看作是超图上节点和边之间的关系。超图上的一个超边对应于这里的一个语义元组。&lt;/p&gt;

&lt;p&gt;然后作者用超图神经网络（Hypergraph Neural Network，HGNN）在这个构建出来的超图上进行学习，然后做一个节点分类任务。超图第$l+1$层的输出计算如下：&lt;/p&gt;

\[\mathbf{Z}^{(l+1)}=\sigma\left(\mathbf{D}_v^{-\frac{1}{2}} \mathbf{H D}_e^{-1} \mathbf{H}^{\top} \mathbf{D}_v^{-\frac{1}{2}} \mathbf{Z}^{(l)} \mathbf{\Psi}^{(l)}\right)\]

&lt;p&gt;$\boldsymbol{\Psi}^{(l)} \in \mathbb{R}^{d_l \times d_{l+1}}$ 表示网络权重，$\boldsymbol{H}$便是这里的$\boldsymbol{S}$，$\mathbf{D}_v$和$\mathbf{D}_e$分别是节点和边的度矩阵。最后一层的输出经过softmax函数得到类别概率，随后计算和真实标签的交叉熵：&lt;/p&gt;

\[\hat{\mathbf{Y}}=\operatorname{softmax}\left(\mathbf{Z}^{(L)}\right)\\
\mathcal{L}_{C E}=-\frac{1}{N_b} \sum_{i=1}^{N_b} \sum_{j=1}^C \mathbf{Y}_{i j} \log \hat{\mathbf{Y}}_{i j}\]

&lt;p&gt;最终模型的损失函数是原型分布的损失和交叉熵损失的加权求和：&lt;/p&gt;

\[\mathcal{L}_{\text {hist }}=\mathcal{L}_D+\lambda_s \mathcal{L}_{C E}\]

&lt;p&gt;模型的框架图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/chthub.github.io/main/imgs/image-20220917151836893.png&quot; alt=&quot;image-20220917151836893&quot; /&gt;&lt;/p&gt;

&lt;p&gt;作者在分析模型框架的合理性时指出，由于超图会使得同一超边里面的样本更加相似，所以为了区分同一超边里面的正样本和误入的负样本，该模型会强迫CNN学习更加有判别性的特征，而不是背景或者噪声信息，这使得学到的特征更具有鲁棒性。&lt;/p&gt;

&lt;h2 id=&quot;实验&quot;&gt;实验&lt;/h2&gt;

&lt;p&gt;在实验部分的激活图挺有意思的：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/chthub.github.io/main/imgs/image-20220917152830000.png&quot; alt=&quot;image-20220917152830000&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;作者讲最后一层卷积层中通道的平均激活值从大到小排序，选出了top-3通道。从结果可以看到，本文提出的HIST使得特征更加集中在目标区域，而且通道有分工，比如那个汽车的图片，有的集中在车前盖，有的集中在车轮，这种通道具有语义信息。其他方法并没有本文的那么显著，有些都集中在背景信息。所以说本文提出的这个方法确实是可以提升模型的特征提取能力，embedding的判别能力，而且更加鲁棒。&lt;/p&gt;

</description>
        <pubDate>Sat, 17 Sep 2022 08:00:00 -0400</pubDate>
        <link>http://localhost:4000/2022/09/17/hyper_metric_learning/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/09/17/hyper_metric_learning/</guid>
        
        <category>度量学习</category>
        
        <category>CVPR</category>
        
        <category>超图</category>
        
        
      </item>
    
      <item>
        <title>Metric learning with adaptive density discrimination</title>
        <description>&lt;p&gt;Published on ICLR 2016&lt;/p&gt;

&lt;p&gt;首先作者认为目前的距离度量学习存在两个大问题：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;基于类别标签的监督信息使得相同类别的样本靠近，不同类别的样本远离。这既没有考虑到类内的不相似性，也没考虑到类间的相似性。&lt;/li&gt;
  &lt;li&gt;现有的Triplet loss或者contrastive loss仅仅惩罚样本对或者样本的三元组，并没有考虑到contextual信息，即周围的邻居结构。这不但增加了计算的复杂度，而且增大了收敛的难度。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/chthub.github.io/main/imgs/image-20220915233113147.png&quot; alt=&quot;image-20220915233113147&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;所以理想的DML算法应该是这样的：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;追求局部的分离，而不是全局的分离。&lt;/li&gt;
  &lt;li&gt;分离表征空间中不同类别的分布&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;所以DML算法要发现不同类别之间的重叠部分，并减少惩罚这种重叠以获得判别。&lt;/p&gt;

&lt;p&gt;所以，需要有聚类算法建模样本的分布。这样可以从直接优化样本转变成优化类簇。同时，基于聚类的算法也降低了负采样的成本，只需要两步，首先先找到相邻的簇，然后再从簇中采样即可。&lt;/p&gt;

&lt;h2 id=&quot;formulation&quot;&gt;Formulation&lt;/h2&gt;

&lt;p&gt;假设现在有数据集包含 $N$ 个输入和标签对 $\mathcal{D}=\left\lbrace \mathbf{x}_n, y_n\right \rbrace _{n=1}^N$ ，标签有 $C$ 类。输入样本$\mathbf{x}_n$的表征向量由映射$\mathbf{f}(\cdot ; \boldsymbol{\Theta})$计算：&lt;/p&gt;

\[\mathbf{r}_n=\mathbf{f}\left(\mathbf{x}_n ; \boldsymbol{\Theta}\right), n=1, \ldots, N\]

&lt;p&gt;对于一组类别为 $c$ 的样本, 利用K-means算法，将其分成$K$簇 $\mathcal{I}_1^c, \ldots, \mathcal{I}_K^c$，目标是要最小化每一簇内样本和其中心点$\mathbf{\mu}^c_k$的距离：&lt;/p&gt;

\[\begin{aligned}
\mathcal{I}_1^c, \ldots, \mathcal{I}_K^c &amp;amp;=\arg \min _{I_1^c, \ldots, I_K^c} \sum_{k=1}^K \sum_{\mathbf{r} \in I_k^c}\left\|\mathbf{r}-\boldsymbol{\mu}_k^c\right\|_2^2 \\
\boldsymbol{\mu}_k^c &amp;amp;=\frac{1}{\left|I_k^c\right|} \sum_{\mathbf{r} \in I_k^c} \mathbf{r}
\end{aligned}\]

&lt;p&gt;定义 $C(\mathbf{r})$ 为表征 $\mathbf{r}$ 的类别标签,  $\boldsymbol{\mu}(\mathbf{r})$ 是其对应的簇心。 则目标函数（Magnet loss）定义如下：&lt;/p&gt;

\[\mathscr{L}(\boldsymbol{\Theta})=\frac{1}{N} \sum_{n=1}^N\left\{-\log \frac{e^{-\frac{1}{2 \sigma^2}\left\|\mathbf{r}_n-\boldsymbol{\mu}\left(\mathbf{r}_n\right)\right\|_2^2-\alpha}}{\sum_{c \neq C\left(\mathbf{r}_n\right)} \sum_{k=1}^K e^{-\frac{1}{2 \sigma^2}\left\|\mathbf{r}_n-\boldsymbol{\mu}_k^c\right\|_2^2}}\right\}_{+}\tag{1}\]

&lt;p&gt;这里$\lbrace\cdot\rbrace_+$表示hinge函数。$\alpha \in \mathbb{R}$，$\sigma^2=\frac{1}{N-1} \sum_{\mathbf{r} \in \mathcal{D}}|\mathbf{r}-\boldsymbol{\mu}(\mathbf{r})|_2^2$表示样本距离其中心的方差。&lt;/p&gt;

&lt;p&gt;直观理解，在分子项，这个目标函数要最小化$\mathbf{r}_n$和$\boldsymbol{\mu}\left(\mathbf{r}_n\right)$之间的距离。对于分母项，该目标函数要最大化$\mathbf{r}_n$和标签与$\mathbf{r}_n$不相同的其他簇心的距离。此外对于分母项来说，和$\mathbf{r}_n$距离较远的簇之间的计算项接近0，可以忽略，所以在实际的计算过程中可以只计算局部邻居就行。这就引出了下文要提到的邻居采样过程。&lt;/p&gt;

&lt;h3 id=&quot;效果演示&quot;&gt;效果演示&lt;/h3&gt;

&lt;p&gt;下图中的白点代表同一个类别里面的不同的簇，这里$K=32$。红箭头表示和那些指出的簇最近的样本。（a）和（b）属于不同类别，但都有动物和人的这一层含义，所虽然属于不同类别，但在空间上更靠近。（b）和（c）属于同一类别Manta，但是（b）表示的是Manta和人，（c）表示的是Manta在深水，所以在空间上距离较远。（c）和（d）属于不同类别，但表示相似的含义（in the deep），所以在空间上距离也更近。可以看到蓝色这一小簇明显是离群了，而这是Triplet和Softmax里面所没有的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/chthub.github.io/main/imgs/image-20220915225956128.png&quot; alt=&quot;image-20220915225956128&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;训练过程&quot;&gt;训练过程&lt;/h2&gt;

&lt;h3 id=&quot;邻居采样&quot;&gt;邻居采样&lt;/h3&gt;

&lt;p&gt;在训练过程中，minbatch构建于局部邻居，而不是随机的独立采样。过程如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/chthub.github.io/main/imgs/image-20220915184410116.png&quot; alt=&quot;image-20220915184410116&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在训练时会保存每个样本的loss，对于某个簇$I$，可以计算这个簇的平均loss，$\mathscr{L}&lt;em&gt;I$。定义$p&lt;/em&gt;{\mathcal{I}}(I) \propto \mathscr{L}_I$，即根据簇loss的大小按概率采样。即簇的loss越大，有更高的概率被采样到。基于簇loss进行采样的一个优点是可能经过几次迭代，所有的簇都能彼此分离，因为在这个过程中会优先优化那些loss很高的簇，这提高了计算效率。而如果随机采样的话，可能会导致多次对同一个簇进行优化，这就降低了计算效率。&lt;/p&gt;

&lt;p&gt;impostor clusters直译是冒名顶替的簇，其实就是指周围的簇。$P_{I_m}(\cdot)$是均匀分布，即簇内的样本是均匀采样的。感觉这里采样簇内样本的设计还有改进的空间。&lt;/p&gt;

&lt;p&gt;然后基于采样的样本可以计算公式（1）的近似版：&lt;/p&gt;

\[\hat{\mathscr{L}}(\boldsymbol{\Theta})=\frac{1}{M D} \sum_{m=1}^M \sum_{d=1}^D\left\{-\log \frac{e^{-\frac{1}{2 \hat{\sigma}^2}\left\|\mathbf{r}_d^m-\hat{\boldsymbol{\mu}}_m\right\|_2^2-\alpha}}{\sum_{\hat{\boldsymbol{\mu}}: C(\hat{\boldsymbol{\mu}}) \neq C\left(\mathbf{r}_d^m\right)} e^{-\frac{1}{2 \hat{\sigma}^2}\left\|\mathbf{r}_d^m-\hat{\boldsymbol{\mu}}\right\|_2^2}}\right\}+\tag{2}\\
\hat{\boldsymbol{\mu}}_m=\frac{1}{D} \sum_{d=1}^D \mathbf{r}_d^m\\
\hat{\sigma}=\frac{1}{M D-1} \sum_{m=1}^M \sum_{d=1}^D\left\|\mathbf{r}_d^m-\hat{\boldsymbol{\mu}}_m\right\|_2^2\]

&lt;p&gt;下图展示了Triplet loss和本文提出的Magnet loss之间的对比。这里有两点对比：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Triplet loss每次只计算一个样本的三元组，而Magnet loss每次计算的是一组局部邻居样本形成的簇。三元组的设计提高了计算的复杂度和计算量。&lt;/li&gt;
  &lt;li&gt;Triplet loss定义的是相似度是基于样本的，Magnet loss定义的相似度是基于簇分布的。Triplet loss惩罚的是样本之间的相似性，而Magnet loss惩罚的是簇之间的重叠度，并没有明确要求样本对之间的距离，所以为建模同类样本内部之间的差异性和不同类样本之间相似性打下了基础。而且，对簇的惩罚使得对样本点的调整更加具有一致性，而Triplet loss里面的三元组并不具有样本间的一致性。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/chthub.github.io/main/imgs/image-20220915185945811.png&quot; alt=&quot;image-20220915185945811&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;评估过程&quot;&gt;评估过程&lt;/h2&gt;

&lt;p&gt;由于数据本身带有标签，每个簇也都有标签，所以可以根据最后的表征向量和邻近的簇之间的距离判断这个样本的标签，用来评估模型的效果。换句话说，样本$\mathbf{x}_n$的标签是其与最近的$L$个簇（$\mathbf{\mu}_1, \ldots, \mathbf{\mu}_L$）的softmax相似度：&lt;/p&gt;

\[c_n^*=\arg \max _{c=1, \ldots, C} \frac{\sum_{\boldsymbol{\mu}_l: C\left(\boldsymbol{\mu}_l\right)=c} e^{-\frac{1}{2 \sigma^2}\left\|\mathbf{r}_n-\boldsymbol{\mu}_l\right\|_2^2}}{\sum_{l=1}^L e^{-\frac{1}{2 \sigma^2}\left\|\mathbf{r}_n-\boldsymbol{\mu}_l\right\|_2^2}}\]

&lt;p&gt;分子是计算样本和类别标签为$c$的所有临近的簇$\mu_l$之间的距离之和。这和$k$-近邻的想法一样。&lt;/p&gt;

&lt;h2 id=&quot;和现有模型的联系&quot;&gt;和现有模型的联系&lt;/h2&gt;

&lt;p&gt;当$M=2$，$D=2$时，公式（2）退化成Triplet loss。&lt;/p&gt;

&lt;h3 id=&quot;neighbourhood-components-analysis&quot;&gt;Neighbourhood Components Analysis&lt;/h3&gt;

&lt;p&gt;NCA的目标函数如下：&lt;/p&gt;

\[\mathscr{L}_{\mathrm{NCA}}(\boldsymbol{\Theta})=\frac{1}{N} \sum_{n=1}^N-\log \frac{\sum_{n^{\prime}: C\left(\mathbf{r}_{n^{\prime}}\right)=C\left(\mathbf{r}_n\right)} e^{-\left\|\mathbf{r}_n-\mathbf{r}_n^{\prime}\right\|_2^2}}{\sum_{n^{\prime}=1}^N e^{-\left\|\mathbf{r}_n-\mathbf{r}_n^{\prime}\right\|_2^2}}\]

&lt;p&gt;可以看到，该目标函数的目的是最小化同一类样本内部的样本点之间的距离。作者指出了该模型的一个问题是缺少一个像本文中的邻居采样过程，计算复杂度较高。&lt;/p&gt;

&lt;h3 id=&quot;nearest-class-mean&quot;&gt;Nearest Class Mean&lt;/h3&gt;

&lt;p&gt;这里的均值向量$\boldsymbol{\mu}_c$是固定住的，可以看到$\boldsymbol{\mu}_c$是根据样本的raw embedding计算的。目的是学习变换$\boldsymbol{W}$。&lt;/p&gt;

\[\mathscr{L}_{\mathrm{NCM}}(\mathbf{W})=\frac{1}{N} \sum_{n=1}^N-\log \frac{e^{-\left\|\mathbf{W} \mathbf{x}_n-\mathbf{W} \boldsymbol{\mu}_{c\left(\mathbf{x}_n\right)}\right\|_2^2}}{\sum_{c=1}^C e^{-\left\|\mathbf{W} \mathbf{x}_n-\mathbf{W} \boldsymbol{\mu}_c\right\|_2^2}}\\
\boldsymbol{\mu}_c=\frac{1}{|C|} \sum_{c(\mathbf{x})=c} \mathbf{x},\; c=1, \ldots, C\]

&lt;h2 id=&quot;实验&quot;&gt;实验&lt;/h2&gt;

&lt;p&gt;在实验细节里面，作者提到在开始训练DML算法之前，先用一个部分训练的softmax warm-start DML模型的权重是一件有用的事情。但是不要用完全训练好的模型权重。所以在本文中，先用一个在ImageNet上训练了3轮的网络来初始化编码器的权重。&lt;/p&gt;

&lt;h3 id=&quot;attribute-distribution&quot;&gt;Attribute distribution&lt;/h3&gt;

&lt;p&gt;在本文中所使用的Magnet可以使不同类别中相似的样本embedding更为接近，同一类别里面不相似的样本距离更远。为了验证这一假设，作者在一个带有属性标签的数据集上进行了实验。如下图（a）所示，除了图片的正常标签外，还有一个attribute 标签，可以看到同一个attrubute标签可以包含不同类别的样本。&lt;/p&gt;

&lt;p&gt;然后作者在训练时完全不使用attribute标签，然后通过计算一个样本的周围邻居中和它属性标签相同的所占比例来评估算法有没有学到这种类间的相似性。下图中的（c）里面显示Magnet loss可以学到这种类别间的相似性。同时，还可以注意到，Softmax的效果要好于Triplet loss。这是因为Triplet loss严格定义了样本间要么相似，要么就不相似，所以很难学到不同类样本间的相似性。而Softmax的简单设计则是为样本之间的相似性留出了空间。&lt;/p&gt;

&lt;p&gt;下图（b）展示了Magnet 和 softmax loss学习之后的embedding，橘色表示同attribute label的样本。可以看到Magnet可以学到跨类别的相似性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/chthub.github.io/main/imgs/image-20220915223912693.png&quot; alt=&quot;image-20220915223912693&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;hierarchy-recovery&quot;&gt;Hierarchy recovery&lt;/h3&gt;

&lt;p&gt;在这个实验中，作者想看看这个算法能不能学习到类别的层次性，能不能识别出类内的差异性。作者将两个不同类别的样本放在一起，用同一个粗化的标签替代它们原来的标签，想看一下是否能从表征中恢复原来的更精细的标签。实验结果如下。可以看到Softmax又一次好于Triplet。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/chthub.github.io/main/imgs/image-20220915225403225.png&quot; alt=&quot;image-20220915225403225&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 15 Sep 2022 08:00:00 -0400</pubDate>
        <link>http://localhost:4000/2022/09/15/metriclearning/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/09/15/metriclearning/</guid>
        
        <category>度量学习</category>
        
        <category>ICLR</category>
        
        
      </item>
    
      <item>
        <title>数学公式测试</title>
        <description>&lt;p&gt;进行数学公式测试，查看渲染结果。&lt;/p&gt;

&lt;p&gt;下面都是得要求下划线后面加空格，否则报错。&lt;/p&gt;

&lt;h2 id=&quot;例一&quot;&gt;例一&lt;/h2&gt;
&lt;p&gt;下划线渲染不正确：&lt;/p&gt;

&lt;p&gt;这里 $\mathbf{h}&lt;em&gt;i^l \in \mathbb{R}^{nf}$ 是节点 $v_i$ 在 $l$ 层的embedding，$a&lt;/em&gt;{i j}$ 表示边的属性，$\mathcal{N}(i)$ 是 $v_i$ 的邻居节点的集合。&lt;/p&gt;

&lt;h2 id=&quot;例二&quot;&gt;例二&lt;/h2&gt;
&lt;p&gt;大括号转义失败：&lt;/p&gt;

&lt;p&gt;$\left{x_i\right}_{i=1}^n$&lt;/p&gt;

&lt;p&gt;应该用&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\lbrace&lt;/code&gt;和&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\rbrace&lt;/code&gt;命令：&lt;/p&gt;

&lt;p&gt;$\left\lbrace x_i\right\rbrace_{i=1}^n$&lt;/p&gt;

&lt;h2 id=&quot;例三&quot;&gt;例三&lt;/h2&gt;
&lt;p&gt;双斜杠换行：&lt;/p&gt;

\[Q\left(c_i\right)=\frac{p\left(x_i, c_i ; \theta\right)}{Const}\\
\sum_{c_i \in C}Q\left(c_i\right)=1=\frac{1}{Const}\sum_{c_i \in C}p\left(x_i, c_i ; \theta\right)\\
Const=\sum_{c_i \in C}p\left(x_i, c_i ; \theta\right)\]

&lt;h2 id=&quot;例四&quot;&gt;例四&lt;/h2&gt;
&lt;p&gt;代码块必须要空行：
\(f(x)=\int f(x)\sum_i^nf(x)\)&lt;/p&gt;

&lt;h2 id=&quot;例五&quot;&gt;例五&lt;/h2&gt;
&lt;p&gt;还是下划线的问题&lt;/p&gt;

&lt;p&gt;损失函数的第一项作者把InfoNCE加入了进来，这里$v’&lt;em&gt;i$是和$v_i$属于相同簇的增广样本，$r$是负样本的数量。总共进行$M$次聚类，每次的类簇数都不一样$K=\left\lbrace k_m\right\rbrace&lt;/em&gt;{m=1}^M$。通过这种方式可以提升原型的概率估计的鲁棒性，并能编码分层的结构。&lt;/p&gt;

&lt;h2 id=&quot;例六&quot;&gt;例六&lt;/h2&gt;

&lt;p&gt;假设现在有数据集包含 $N$ 个输入和标签对 $\mathcal{D}=\left\lbrace \mathbf{x}&lt;em&gt;n, y_n\right\rbrace&lt;/em&gt;{n=1}^N$ ，标签有 $C$ 类。输入样本$\mathbf{x}_n$的表征向量由映射$\mathbf{f}(\cdot ; \boldsymbol{\Theta})$计算：&lt;/p&gt;
</description>
        <pubDate>Thu, 15 Sep 2022 08:00:00 -0400</pubDate>
        <link>http://localhost:4000/2022/09/15/mathtest/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/09/15/mathtest/</guid>
        
        
      </item>
    
      <item>
        <title>Prototypical Networks for Few-shot Learning</title>
        <description>&lt;p&gt;Published on NeurIPS 2017&lt;/p&gt;

&lt;p&gt;这篇文章提出了原型网络（prototypical networks）用于小样本的分类问题。&lt;/p&gt;

&lt;p&gt;如下图所示，该模型可以应用到小样本或零样本的学习中。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/everydaypaper/main/imgs/image-20220913104436205.png&quot; alt=&quot;image-20220913104436205&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;formulation&quot;&gt;Formulation&lt;/h2&gt;

&lt;p&gt;在小样本分类中，我们给定了一个小的有标签的支撑集（support set）$S=\left\lbrace \left(\mathbf{x}_1, y_1\right), \ldots,\left(\mathbf{x}_N, y_N\right)\right\rbrace $，$\mathbf{x}_i\in \mathbb{R}^D$, $y_i\in \lbrace 1,\dots,K\rbrace $是对应的标签。$S_k$表示类别$k$的有标签的样本集合。&lt;/p&gt;

&lt;p&gt;原型网络为每一个类别通过嵌入函数 $f_\phi: \mathbb{R}^D \rightarrow \mathbb{R}^M$计算一个 $M$维的表征 $\mathbf{c}_k \in \mathbb{R}^M$, 也称为原型。这里可学习的参数是 $\phi$，计算方式如下：&lt;/p&gt;

\[\mathbf{c}_k=\frac{1}{\left|S_k\right|} \sum_{\left(\mathbf{x}_i, y_i\right) \in S_k} f_\phi\left(\mathbf{x}_i\right) \tag{1}\]

&lt;p&gt;给定一个距离函数 $d: \mathbb{R}^M \times \mathbb{R}^M \rightarrow[0,+\infty)$，基于查询点和原型之间的距离，原型网络将会输出查询点$\mathbf{x}$在所有类别上的分布：&lt;/p&gt;

\[p_\phi(y=k \mid \mathbf{x})=\frac{\exp \left(-d\left(f_\phi(\mathbf{x}), \mathbf{c}_k\right)\right)}{\sum_{k^{\prime}} \exp \left(-d\left(f_\phi(\mathbf{x}), \mathbf{c}_{k^{\prime}}\right)\right)}\tag{2}\]

&lt;p&gt;损失函数是：&lt;/p&gt;

\[J(\phi)=-\log p_\phi(y=k \mid \mathbf{x})\tag{3}\]

&lt;p&gt;$k$是$\mathbf{x}$的真实类别。&lt;/p&gt;

&lt;h2 id=&quot;伪代码&quot;&gt;伪代码&lt;/h2&gt;

&lt;p&gt;伪代码里面的损失函数就是把上面的损失函数代入，展开之后的结果。直观理解有个距离项，让查询点$\mathbf{x}$离其所属类别的原型更近，离其他类别的原型更远。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/everydaypaper/main/imgs/image-20220913101913210.png&quot; alt=&quot;image-20220913101913210&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;regular-bregman-divergences&quot;&gt;regular Bregman divergences&lt;/h2&gt;

&lt;p&gt;这一节讨论了原型网络和regular Bregman divergences之间的关系，还有待于进一步学习。&lt;/p&gt;

&lt;p&gt;此外，余弦距离不属于Bregman divergences，不满足这里的假设，所以在实验中也发现，欧氏距离要优于余弦距离。&lt;/p&gt;

&lt;h2 id=&quot;欧式距离等价于线性模型&quot;&gt;欧式距离等价于线性模型&lt;/h2&gt;

&lt;p&gt;如果取$d\left(\mathbf{z}, \mathbf{z}^{\prime}\right)=\left|\mathbf{z}-\mathbf{z}^{\prime}\right|^2$，那么公式（2）等价于线性模型：&lt;/p&gt;

\[-\left\|f_\phi(\mathbf{x})-\mathbf{c}_k\right\|^2=-f_\phi(\mathbf{x})^{\top} f_\phi(\mathbf{x})+2 \mathbf{c}_k^{\top} f_\phi(\mathbf{x})-\mathbf{c}_k^{\top} \mathbf{c}_k\]

&lt;p&gt;第一项和类别$k$无关，是常数项。所以可以重写为：&lt;/p&gt;

\[2 \mathbf{c}_k^{\top} f_\phi(\mathbf{x})-\mathbf{c}_k^{\top} \mathbf{c}_k=\mathbf{w}_k^{\top} f_\phi(\mathbf{x})+b_k, \text { where } \mathbf{w}_k=2 \mathbf{c}_k \text { and } b_k=-\mathbf{c}_k^{\top} \mathbf{c}_k\]

&lt;p&gt;尽管这里是一个线性函数的形式，但是$f_\phi$可以学到非线性的表征，所以证明了欧氏距离的作用。&lt;/p&gt;

&lt;h2 id=&quot;和匹配网络matching-networks的比较&quot;&gt;和匹配网络（Matching Networks）的比较&lt;/h2&gt;

&lt;p&gt;在one-shot学习中，$\mathbf{c}_k=\mathbf{x}_k$，匹配网络等价于原型网络。&lt;/p&gt;

&lt;p&gt;一个很自然的问题是在每一类中使用多个原型，而不是一个的情况是不是有意义的。这需要一种划分策略，在下面两篇文中有提及。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Distance-based image classification: Generalizing to new classes at near-zero cost&lt;/li&gt;
  &lt;li&gt;Metric learning with adaptive density discrimination&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;零样本学习&quot;&gt;零样本学习&lt;/h2&gt;

&lt;p&gt;在零样本学习中，没有支撑集，对于每一类只有一个类的元数据向量$\mathbf{v}&lt;em&gt;k$。为了使用原型网络，直接定义$\mathbf{c}_k=g&lt;/em&gt;{\vartheta}\left(\mathbf{v}_k\right)$。&lt;/p&gt;
</description>
        <pubDate>Mon, 12 Sep 2022 08:00:00 -0400</pubDate>
        <link>http://localhost:4000/2022/09/12/prototypicalnetwork/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/09/12/prototypicalnetwork/</guid>
        
        <category>小样本学习</category>
        
        <category>度量学习</category>
        
        <category>NeurIPS</category>
        
        
      </item>
    
      <item>
        <title>Prototypical Contrastive Learning of unsupervised representations</title>
        <description>&lt;p&gt;Published on ICLR 2021&lt;/p&gt;

&lt;p&gt;这篇文章提出了Prototypical Contrastive Learning （PCL），这是一种无监督的表示学习算法，将对比学习和聚类联系在一起。&lt;/p&gt;

&lt;p&gt;作者认为instance-wise contrastive learning存在两个问题：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;实例之间的判别只利用到了低价的图像之间的差异，而没有考虑到高阶的语义关系&lt;/li&gt;
  &lt;li&gt;只要求正负样本远离，没有考虑到负样本内部的相似性。不可避免地会有负样本对具有相似的语义，本应在表征空间中相距更近，但被对比损失拉远了。这是一种class collision问题。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;formulation&quot;&gt;Formulation&lt;/h2&gt;

&lt;p&gt;最终的目标是找到能使观察到的$n$个样本的对数似然最大化的参数$\theta$：&lt;/p&gt;

\[\theta^*=\underset{\theta}{\arg \max } \sum_{i=1}^n \log p\left(x_i ; \theta\right)\]

&lt;p&gt;假设观测到的数据集是 $\left\lbrace x_i\right\rbrace_{i=1}^n$， 对应的原型（prototype）是隐变量 $C=\left\lbrace c_i\right\rbrace_{i=1}^k$ ，则代入$C$上式化为：&lt;/p&gt;

\[\theta^*=\underset{\theta}{\arg \max } \sum_{i=1}^n \log p\left(x_i ; \theta\right)=\underset{\theta}{\arg \max } \sum_{i=1}^n \log \sum_{c_i \in C} p\left(x_i, c_i ; \theta\right)\]

&lt;p&gt;现在还是很难优化，利用Jensen’s inequality，最大化上式可以转化为最大化上式的下界：&lt;/p&gt;

\[\begin{align}
\sum_{c_i \in C} Q\left(c_i\right)&amp;amp;=1 \\
\sum_{i=1}^n \log \sum_{c_i \in C} p\left(x_i, c_i ; \theta\right) &amp;amp;=\sum_{i=1}^n \log \sum_{c_i \in C} Q\left(c_i\right) \frac{p\left(x_i, c_i ; \theta\right)}{Q\left(c_i\right)}\\
&amp;amp; \geq \sum_{i=1}^n \sum_{c_i \in C} Q\left(c_i\right) \log \frac{p\left(x_i, c_i ; \theta\right)}{Q\left(c_i\right)}\\
&amp;amp;=\sum_{i=1}^n \sum_{c_i \in C} Q\left(c_i\right) \log p\left(x_i, c_i ; \theta\right)-\underbrace{\sum_{i=1}^n \sum_{c_i \in C} Q\left(c_i\right) \log Q\left(c_i\right)}_{\text{constant}}
\end{align}\]

&lt;p&gt;由于第二项为常数，所以只要最大化第一项就行了：&lt;/p&gt;

\[\theta^*=\underset{\theta}{\arg \max }\sum_{i=1}^n \sum_{c_i \in C} Q\left(c_i\right) \log p\left(x_i, c_i ; \theta\right)\]

&lt;p&gt;当$\frac{p\left(x_i, c_i ; \theta\right)}{Q\left(c_i\right)}$为常数时等号成立，有：&lt;/p&gt;

\[Q\left(c_i\right)=\frac{p\left(x_i, c_i ; \theta\right)}{Const}\\
\sum_{c_i \in C}Q\left(c_i\right)=1=\frac{1}{Const}\sum_{c_i \in C}p\left(x_i, c_i ; \theta\right)\\
Const=\sum_{c_i \in C}p\left(x_i, c_i ; \theta\right)\]

&lt;p&gt;所以此时代入$Const$可以发现$Q(c_i)$就是在$x_i$条件下的后验概率$p(c_i;x_i,\theta)$：&lt;/p&gt;

\[Q\left(c_i\right)=\frac{p\left(x_i, c_i ; \theta\right)}{\sum_{c_i \in C} p\left(x_i, c_i ; \theta\right)}=\frac{p\left(x_i, c_i ; \theta\right)}{p\left(x_i ; \theta\right)}=p\left(c_i ; x_i, \theta\right)\]

&lt;p&gt;然后使用EM算法求解。&lt;/p&gt;

&lt;p&gt;在E-step，估计$Q(c_i)$也即$p\left(c_i ; x_i, \theta\right)$。对特征$v_i^{\prime}=f_{\theta^{\prime}}\left(x_i\right)$ 使用 $k$-means 聚类形成$k$个簇，这里的$f_{\theta’}$是动量编码器。原型$c_i$定义为第$i$个簇的簇心。$Q(c_i)$定义如下：&lt;/p&gt;

\[Q(c_i)=p\left(c_i ; x_i, \theta\right)=\mathbb{1}\left(x_i \in c_i\right)=\left\lbrace 
\begin{aligned}
1, x_i\in c_i\\
0, x_i \notin c_i
\end{aligned}
\right.\]

&lt;p&gt;在M-step进行最大化：&lt;/p&gt;

\[\begin{aligned}
\underset{\theta}{\arg \max }\sum_{i=1}^n \sum_{c_i \in C} Q\left(c_i\right) \log p\left(x_i, c_i ; \theta\right) &amp;amp;=\sum_{i=1}^n \sum_{c_i \in C} p\left(c_i ; x_i, \theta\right) \log p\left(x_i, c_i ; \theta\right) \\
&amp;amp;=\sum_{i=1}^n \sum_{c_i \in C} \mathbb{1}\left(x_i \in c_i\right) \log p\left(x_i, c_i ; \theta\right)\\
&amp;amp;=\sum_{i=1}^n \sum_{c_i \in C} \mathbb{1}\left(x_i \in c_i\right) \log p\left(x_i ; c_i, \theta\right) \underbrace{p\left(c_i ; \theta\right)}_{\text{uniform prior}} \\
&amp;amp;=\sum_{i=1}^n \sum_{c_i \in C} \mathbb{1}\left(x_i \in c_i\right) \log\frac{1}{k} \cdot p\left(x_i ; c_i, \theta\right)
\end{aligned}\]

&lt;p&gt;假设原型$c_i$周围的样本点服从各向同性的高斯分布（isotropic Gaussian）：&lt;/p&gt;

\[p\left(x_i ; c_i, \theta\right)=\exp \left(\frac{-\left(v_i-c_s\right)^2}{2 \sigma_s^2}\right) / \sum_{j=1}^k \exp \left(\frac{-\left(v_i-c_j\right)^2}{2 \sigma_j^2}\right)\]

&lt;p&gt;这里$v_i=f_\theta\left(x_i\right)$ ，$x_i \in c_s$。注意到这里有一个$c_s$和$c_i$之间的差异，因为计算的时候是对所有的样本点和所有原型之间进行计算，所以这里的$c_s$专指$x_i$所属于的那个原型。此外，这里的每一个簇的方差$\sigma_j$也是不同的。这里还有一点需要注意的是$v_i$和距离$v_i$比较远的原型$c_i$之间计算得到的相似性很低，在分母中只占较小的比重，所以这个式子也可以近似看作是只考虑了距离$v_i$最近的几个簇心后的概率分布。&lt;/p&gt;

&lt;p&gt;最终化简之后的目标是要最小化下式：&lt;/p&gt;

\[\theta^*=\underset{\theta}{\arg \min } \sum_{i=1}^n-\log \frac{\exp \left(v_i \cdot c_s / \phi_s\right)}{\sum_{j=1}^k \exp \left(v_i \cdot c_j / \phi_j\right)}\]

&lt;p&gt;这里$\phi \propto \sigma^2$表示样本点在原型周围分布的聚集程度。注意到最终的优化目标和InfoNCE相似，所以说InfoNCE也可以解释成一种极大似然估计的特殊情况。&lt;/p&gt;

&lt;p&gt;最终的损失函数定义如下：&lt;/p&gt;

\[\mathcal{L}_{\text {ProtoNCE }}=\sum_{i=1}^n-\left(\log \frac{\exp \left(v_i \cdot v_i^{\prime} / \tau\right)}{\sum_{j=0}^r \exp \left(v_i \cdot v_j^{\prime} / \tau\right)}+\frac{1}{M} \sum_{m=1}^M \log \frac{\exp \left(v_i \cdot c_s^m / \phi_s^m\right)}{\sum_{j=0}^r \exp \left(v_i \cdot c_j^m / \phi_j^m\right)}\right)\]

&lt;p&gt;损失函数的第一项作者把InfoNCE加入了进来，这里$v’_i$是和$v_i$属于相同簇的增广样本，$r$是负样本的数量。总共进行$M$次聚类，每次的类簇数都不一样$K=\left\lbrace k_m\right \rbrace _{m=1}^M$。通过这种方式可以提升原型的概率估计的鲁棒性，并能编码分层的结构。&lt;/p&gt;

&lt;p&gt;$\phi$表示样本点的集中程度，使用同一簇的动量特征$\left\lbrace v_z^{\prime}\right\rbrace_{z=1}^Z$估计$\phi$。$\phi$越小，表明聚集程度越大。对于较小的$\phi$，应该满足：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$v’_z$和$c$之间的平均距离小&lt;/li&gt;
  &lt;li&gt;簇包含的特征点多（$Z$比较大）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;所以，定义如下：&lt;/p&gt;

\[\phi=\frac{\sum_{z=1}^Z\left\|v_z^{\prime}-c\right\|_2}{Z \log (Z+\alpha)}\]

&lt;p&gt;$\alpha$是平滑系数，确保$\phi$不会太大。$\tau$通过在原型集合$C^m$上对$\phi$归一化并取均值得到。&lt;/p&gt;

&lt;p&gt;对于松散的簇（$\phi$较大）来说，ProtoNCE会拉近样本embedding和原型之间距离。对于紧凑的簇（$\phi$较小）来说，ProtoNCE更少地使样本的embedding接近原型。所以ProtoNCE将会产生更加平衡的簇，且簇的聚集程度都较为相似。这还阻止了所有embedding都坍塌到同一个簇的这种平凡解（DeepCluster里面通过数据重采样解决了这个问题）。如下图所示，引入对聚集程度的估计使得簇的大小分布更为集中，固定的聚集程度的簇的大小更分散。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/everydaypaper/main/imgs/image-20220911003937990.png&quot; alt=&quot;image-20220911003937990&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下图是模型的框架图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/chthub.github.io/main/imgs/image-20220915191737125.png&quot; alt=&quot;image-20220915191737125&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;互信息分析&quot;&gt;互信息分析&lt;/h2&gt;

&lt;p&gt;我们已经知道最小化InfoNCE等价于最大化$V$和$V’$之间互信息。相似地，最小化本文提到的ProtoNCE等价于最大化$V$和$\lbrace V’,C^1,\dots,C^M\rbrace$之间的互信息。这带来的优势有两方面：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;这种情况下编码器能够学习到原型之间共享信息，这就忽略了原型内部的噪声，使得这种共享的信息更可能捕获高阶的语义知识。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;和样本特征相比，原型和类别标签之间的互信息更大。如下图所示，在训练过程中，embedding和类别标签之间的互信息是不断增加的。但是可以看到原型和类别标签之间的互信息是最大的，单纯的样本特征和类别标签的互信息是比较小的。ProtoNCE带来的互信息增益是大于InfoNCE的。这说明原型确实是学到了更丰富的语义。&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/everydaypaper/main/imgs/image-20220911005020940.png&quot; alt=&quot;image-20220911005020940&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

    &lt;h2 id=&quot;实验&quot;&gt;实验&lt;/h2&gt;

    &lt;p&gt;在训练过程中有一个需要注意的细节是作者先只用InfoNCE训练模型20轮以warm-up，然后再用本文中的loss。&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/everydaypaper/main/imgs/image-20220911010552285.png&quot; alt=&quot;image-20220911010552285&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;这张图片展示了该算法可以学到不同粒度的高阶语义。这里的不同粒度是由聚类时预先指定的类簇数$k$所决定的，$k$越大，粒度越细，反之粒度则粗。下图中的绿色和黄色也是表示不同的粒度，但具有相似也有差别的语义关系。&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/everydaypaper/main/imgs/image-20220911010951177.png&quot; alt=&quot;image-20220911010951177&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/everydaypaper/main/imgs/image-20220911011019443.png&quot; alt=&quot;image-20220911011019443&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

    &lt;h2 id=&quot;伪代码&quot;&gt;伪代码&lt;/h2&gt;

    &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/everydaypaper/main/imgs/image-20220911005531307.png&quot; alt=&quot;image-20220911005531307&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 11 Sep 2022 08:00:00 -0400</pubDate>
        <link>http://localhost:4000/2022/09/11/pcl/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/09/11/pcl/</guid>
        
        <category>对比学习</category>
        
        <category>度量学习</category>
        
        <category>ICLR</category>
        
        
      </item>
    
      <item>
        <title>E(n) Equivariant Graph Neural Networks</title>
        <description>&lt;p&gt;Published on ICML 2021&lt;/p&gt;

&lt;p&gt;这篇文章提出一种对平移，旋转，反射和排列等变的图神经网络 E(n)-Equivariant Graph Neural Networks&lt;/p&gt;

&lt;h2 id=&quot;formulation&quot;&gt;Formulation&lt;/h2&gt;

&lt;p&gt;几何等变的图神经网络和一般的图神经网络的区别在于几何等变图神经网络在消息传递的过程中考虑到了空间坐标信息，同时要保持坐标信息对于空间变换的等变性。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/everydaypaper/main/imgs/image-20220909134601099.png&quot; alt=&quot;image-20220909134601099&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在这张图里面，对原图（左上角）进行旋转，节点的embedding没有发生变化，但是坐标的embedding变了。同时分别对原图和旋转之后的图消息传递之后得到的新的坐标的embedding来说，也存在一个等价的旋转变换。所以这里的等变性是一种约束，约束坐标的embedding不能随意改变，坐标的embedding要满足这种空间上的等变性。换句话说，我们希望学到的这种坐标的embedding确实是代表了空间位置信息，而不是其他的含义。此外，对于节点自身的embedding来说，它也能在更新的时候加入空间信息，使得节点的embedding更能具有空间上的唯一性，这一点在后面在自编码器的例子上的核心想法。&lt;/p&gt;

&lt;p&gt;给定图 $\mathcal{G}=(\mathcal{V}, \mathcal{E})$，节点 $v_i \in \mathcal{V}$，边 $e_{i j} \in \mathcal{E}$，图卷积的消息传递过程如下：&lt;/p&gt;

\[\begin{aligned}
\mathbf{m}_{i j} &amp;amp;=\phi_e\left(\mathbf{h}_i^l, \mathbf{h}_j^l, a_{i j}\right) \\
\mathbf{m}_i &amp;amp;=\sum_{j \in \mathcal{N}(i)} \mathbf{m}_{i j} \\
\mathbf{h}_i^{l+1} &amp;amp;=\phi_h\left(\mathbf{h}_i^l, \mathbf{m}_i\right)
\end{aligned}\]

&lt;p&gt;这里 $\mathbf{h}_ i^l \in \mathbb{R}^{nf}$ 是节点 $v_i$ 在 $l$ 层的embedding，$a_{i j}$ 表示边的属性，$\mathcal{N}(i)$ 是 $v_i$ 的邻居节点的集合。&lt;/p&gt;

&lt;p&gt;本文中提出的Equivariant Graph Convolutional Layer (EGCL)的消息传递过程如下：&lt;/p&gt;

\[\begin{aligned}
\mathbf{h}^{l+1}, \mathbf{x}^{l+1}&amp;amp;=\operatorname{EGCL}\left[\mathbf{h}^l, \mathbf{x}^l, \mathcal{E}\right]\\
\mathbf{m}_{i j} &amp;amp;=\phi_e\left(\mathbf{h}_i^l, \mathbf{h}_j^l,\left\|\mathbf{x}_i^l-\mathbf{x}_j^l\right\|^2, a_{i j}\right) \\
\mathbf{x}_i^{l+1} &amp;amp;=\mathbf{x}_i^l+C \sum_{j \neq i}\left(\mathbf{x}_i^l-\mathbf{x}_j^l\right) \phi_x\left(\mathbf{m}_{i j}\right),\; C=\frac{1}{|\mathcal{V}|-1} \\
\mathbf{m}_i &amp;amp;=\sum_{j \neq i} \mathbf{m}_{i j} \\
\mathbf{h}_i^{l+1} &amp;amp;=\phi_h\left(\mathbf{h}_i^l, \mathbf{m}_i\right)
\end{aligned}\]

&lt;p&gt;这里的$\mathbf{x}_i^l$表示节点$v_i$在$l$层的坐标的embedding。区别在于等变的GNN里面的$\mathbf{m}$引入了坐标信息，这里的坐标的embedding也是在每层都会更新。&lt;/p&gt;

&lt;p&gt;这里有一个细节是$\mathbf{x}_i^{l+1}$的计算时考虑到了除$v_i$之外的所有其他节点，而不只是$v_i$的邻居。同时$\mathbf{m}$在聚集的时候也是在整个图上聚集而不是局限在邻居节点。&lt;/p&gt;

&lt;p&gt;此外，在消息传递的时候还可以把节点的速度考虑进去，初始速度是$\mathbf{v}_i^{\text{init}}$：&lt;/p&gt;

\[\begin{aligned}
\mathbf{h}^{l+1}, \mathbf{x}^{l+1}, \mathbf{v}^{l+1}&amp;amp;=\operatorname{EGCL}\left[\mathbf{h}^l, \mathbf{x}^l, \mathbf{v}^{\text {init }}, \mathcal{E}\right]\\
&amp;amp;\mathbf{v}_i^{l+1}=\phi_v\left(\mathbf{h}_i^l\right) \mathbf{v}_i^{\text {init }}+C \sum_{j \neq i}\left(\mathbf{x}_i^l-\mathbf{x}_j^l\right) \phi_x\left(\mathbf{m}_{i j}\right) \\
&amp;amp;\mathbf{x}_i^{l+1}=\mathbf{x}_i^l+\mathbf{v}_i^{l+1}
\end{aligned}\]

&lt;p&gt;注意到如果$\mathbf{v}_i^{\text{init}}=0$，那么这个式子和前面没有速度项的消息传递一样。&lt;/p&gt;

&lt;h2 id=&quot;链接预测&quot;&gt;链接预测&lt;/h2&gt;

&lt;p&gt;文章中的这个模型可以用来做链接预测任务。如果一开始只提供了一个点云或者一个节点集合，此时如果以全连接的方式表示变，这种方法不能scale到节点数量很多的情况。本文提出了一种同时将消息传递和边的构建结合在一起的方法：&lt;/p&gt;

\[\mathbf{m}_i=\sum_{j \in \mathcal{N}(i)} \mathbf{m}_{i j}=\sum_{j \neq i} e_{i j} \mathbf{m}_{i j}\\
e_{i j} \approx \phi_{i n f}\left(\mathbf{m}_{i j}\right),\; \phi_{i n f}: \mathbb{R}^{n f} \rightarrow[0,1]^1\]

&lt;p&gt;$\phi_{i n f}$就是一个带有sigmoid函数的线性层，对连边进行概率估计。&lt;/p&gt;

&lt;h2 id=&quot;n-body-system&quot;&gt;N-body system&lt;/h2&gt;

&lt;p&gt;给定N个带点粒子，已知初始的电荷量，位置和速度，预测1000个时间戳后的位置。回归问题&lt;/p&gt;

&lt;h2 id=&quot;图自编码器&quot;&gt;图自编码器&lt;/h2&gt;

&lt;p&gt;图自编码器的解码器计算邻接矩阵出了直接内积恢复之外，还可以这样定义：&lt;/p&gt;

\[\hat{A}_{i j}=g_e\left(\mathbf{z}_i, \mathbf{z}_j\right)=\frac{1}{1+\exp \left(w\left\|\mathbf{z}_i-\mathbf{z}_j\right\|^2+b\right)}\]

&lt;p&gt;直接内积的方式适合图变分自编码器，这种方式对GAE和VGAE都挺合适的。&lt;/p&gt;

&lt;p&gt;然后是一个一般的图神经网络无法处理的一种情况：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/everydaypaper/main/imgs/image-20220909142349775.png&quot; alt=&quot;image-20220909142349775&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于这种4个节点连接起来的cycle graph，如果每个节点的embedding都是一样的，那么消息传递之后的节点embedding也是一样的，自编码器就无法重构节点之间的连边。&lt;/p&gt;

&lt;p&gt;一种解决方法是在节点的embedding里面加入噪声，在这个例子里对每个节点的embedding加入了从$\mathcal{N}(\mathbf{0}, \sigma \mathbf{I})$中采样的随机噪声。这种方法的缺点是引入了额外的噪声分布，使得模型也不得不去学习这个分布。&lt;/p&gt;

&lt;p&gt;另一种方法是在节点的embedding里面加入位置信息。(Paper: on the equivalence between positional node embeddings and structural graph representations)&lt;/p&gt;

&lt;p&gt;然而在本文中，作者将随机采样的噪声作为位置坐标输入到模型里面，输出的节点embedding用于重构邻接矩阵。&lt;/p&gt;

&lt;p&gt;实验结果如下，Noise-GNN就是在embedding里面加了随机噪声的GNN模型，右图是尝试过拟合训练集，pe是指训练集图的稀疏程度：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/everydaypaper/main/imgs/image-20220909143644002.png&quot; alt=&quot;image-20220909143644002&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到EGNN的power是比Noise-GNN更强的。&lt;/p&gt;

&lt;p&gt;还有一个实验是两个数据集上embedding维度的影响：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chthub/everydaypaper/main/imgs/image-20220909144035900.png&quot; alt=&quot;image-20220909144035900&quot; /&gt;&lt;/p&gt;

&lt;p&gt;还是维度越高，结果越好。&lt;/p&gt;
</description>
        <pubDate>Fri, 09 Sep 2022 08:00:00 -0400</pubDate>
        <link>http://localhost:4000/2022/09/09/EquivariantGraphNeuralNetworks/</link>
        <guid isPermaLink="true">http://localhost:4000/2022/09/09/EquivariantGraphNeuralNetworks/</guid>
        
        <category>等变图神经网络</category>
        
        <category>ICML</category>
        
        
      </item>
    
  </channel>
</rss>
